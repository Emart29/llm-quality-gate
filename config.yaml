llm:
  default_provider: "groq"
  temperature: 0.0
  max_tokens: 1000
  timeout: 30

providers:
  # Free/Open-source hosted providers
  groq:
    api_key_env: "GROQ_API_KEY"
    model: "llama-3.1-8b-instant"
    base_url: "https://api.groq.com/openai/v1"
  
  huggingface:
    api_key_env: "HUGGINGFACE_API_KEY"
    model: "microsoft/DialoGPT-medium"
    timeout: 60
  
  openrouter:
    api_key_env: "OPENROUTER_API_KEY"
    model: "microsoft/wizardlm-2-8x22b"
    base_url: "https://openrouter.ai/api/v1"
    site_url: "https://llm-quality-gate"
    app_name: "LLM Quality Gate"
  
  # Proprietary APIs
  openai:
    api_key_env: "OPENAI_API_KEY"
    model: "gpt-3.5-turbo"
    base_url: "https://api.openai.com/v1"
  
  claude:
    api_key_env: "ANTHROPIC_API_KEY"
    model: "claude-3-haiku-20240307"
    base_url: "https://api.anthropic.com"
  
  gemini:
    api_key_env: "GOOGLE_API_KEY"
    model: "gemini-1.5-flash"
  
  # Local providers (optional)
  ollama:
    model: "llama2"
    base_url: "http://localhost:11434"
    timeout: 120
  
  localai:
    model: "gpt-3.5-turbo"
    base_url: "http://localhost:8080"
    api_key_env: "LOCALAI_API_KEY"  # Optional
    timeout: 120

# Separate Generator and Judge roles
roles:
  generator:
    provider: "groq"  # Fast, free for generation
    model: "llama-3.1-8b-instant"
  
  judge:
    provider: "openai"  # More reliable for evaluation
    model: "gpt-3.5-turbo"

evaluation:
  max_retries: 3
  retry_delay: 1.0
  parallel_requests: 5
  consistency_runs: 3  # For stability metric
  ci_lightweight_mode: true  # Enable lightweight mode for CI environments
  
quality_gates:
  task_success_threshold: 0.8
  relevance_threshold: 0.7
  hallucination_threshold: 0.1
  consistency_threshold: 0.8