# ============================================================================
# LLMQ Configuration Example
# ============================================================================
# 
# This is a comprehensive example configuration for LLMQ.
# Copy this file to `config.yaml` and customize for your needs.
#
# Quick start:
#   1. Copy: cp config.example.yaml config.yaml
#   2. Set API keys in .env file (see .env.example)
#   3. Run: llmq dashboard
#
# ============================================================================

# ── LLM Global Settings ────────────────────────────────────────────────────
llm:
  # Default provider when none specified
  default_provider: "groq"
  
  # Global temperature (0.0 = deterministic, 1.0 = creative)
  # Recommended: 0.0 for consistent evaluation results
  temperature: 0.0
  
  # Maximum tokens for LLM responses
  max_tokens: 1000
  
  # Request timeout in seconds
  timeout: 30

# ── LLM Provider Configurations ────────────────────────────────────────────
providers:
  
  # ── Free/Open-source Providers ──
  
  # Groq - Fast inference for open models (Recommended for getting started)
  # Get API key: https://console.groq.com/keys
  groq:
    api_key_env: "GROQ_API_KEY"           # Environment variable name
    model: "llama3-8b-8192"               # Default model
    base_url: "https://api.groq.com/openai/v1"
    # Free tier: 30 req/min, 6000 req/day
  
  # HuggingFace - Access to thousands of open models
  # Get API key: https://huggingface.co/settings/tokens
  huggingface:
    api_key_env: "HUGGINGFACE_API_KEY"
    model: "microsoft/DialoGPT-medium"
    timeout: 60                           # Longer timeout for slower models
  
  # OpenRouter - Multi-model access through one API
  # Get API key: https://openrouter.ai/keys
  openrouter:
    api_key_env: "OPENROUTER_API_KEY"
    model: "microsoft/wizardlm-2-8x22b"
    base_url: "https://openrouter.ai/api/v1"
    site_url: "https://llm-quality-gate"
    app_name: "LLM Quality Gate"
  
  # ── Proprietary Providers ──
  
  # OpenAI - GPT models (Recommended for production)
  # Get API key: https://platform.openai.com/api-keys
  openai:
    api_key_env: "OPENAI_API_KEY"
    model: "gpt-3.5-turbo"                # Cost-effective option
    # model: "gpt-4"                      # Higher quality, more expensive
    base_url: "https://api.openai.com/v1"
  
  # Anthropic Claude - Safety-focused models
  # Get API key: https://console.anthropic.com/
  claude:
    api_key_env: "ANTHROPIC_API_KEY"
    model: "claude-3-haiku-20240307"      # Fast and cost-effective
    # model: "claude-3-sonnet-20240229"   # Balanced performance
    # model: "claude-3-opus-20240229"     # Highest quality
    base_url: "https://api.anthropic.com"
  
  # Google Gemini - Multimodal capabilities
  # Get API key: https://aistudio.google.com/app/apikey
  gemini:
    api_key_env: "GOOGLE_API_KEY"
    model: "gemini-1.5-flash"             # Fast and efficient
    # model: "gemini-1.5-pro"             # Higher quality
  
  # ── Local/Self-hosted Providers ──
  
  # Ollama - Run models locally (No API key needed)
  # Install: https://ollama.ai/
  ollama:
    model: "llama2"                       # Download with: ollama pull llama2
    base_url: "http://localhost:11434"
    timeout: 120                          # Local inference can be slower
  
  # LocalAI - Self-hosted OpenAI-compatible API
  # Setup: https://localai.io/
  localai:
    model: "gpt-3.5-turbo"                # Model name in LocalAI
    base_url: "http://localhost:8080"
    api_key_env: "LOCALAI_API_KEY"        # Optional, if auth enabled
    timeout: 120

# ── Role-based Provider Assignment ─────────────────────────────────────────
# Assign different providers for different tasks
roles:
  # Generator: Creates responses to prompts
  # Recommendation: Use fast, free provider for generation
  generator:
    provider: "groq"
    model: "llama3-8b-8192"
  
  # Judge: Evaluates responses for quality metrics
  # Recommendation: Use reliable, high-quality provider for evaluation
  judge:
    provider: "openai"
    model: "gpt-3.5-turbo"

# ── Evaluation Settings ────────────────────────────────────────────────────
evaluation:
  # Number of retry attempts for failed requests
  max_retries: 3
  
  # Delay between retries (seconds)
  retry_delay: 1.0
  
  # Number of parallel requests to make
  # Increase for faster evaluation, decrease if hitting rate limits
  parallel_requests: 5
  
  # Number of runs for consistency metric
  # Higher = more accurate consistency measurement, slower evaluation
  consistency_runs: 3
  
  # CI Lightweight Mode - Automatically enabled in CI environments
  # When enabled:
  # - Uses mock embeddings instead of real sentence-transformers
  # - Reduces consistency runs to 2 maximum
  # - Skips heavy similarity computations
  # - Uses heuristic-based metrics for faster execution
  # Set to true to force lightweight mode even outside CI
  ci_lightweight_mode: false

# ── Quality Gate Thresholds ────────────────────────────────────────────────
# These thresholds determine pass/fail for quality gates
# Adjust based on your quality requirements
quality_gates:
  
  # Task Success: Does the output match the expected answer?
  # Range: 0.0 - 1.0 (higher = stricter)
  # Recommended: 0.8 for most applications
  task_success_threshold: 0.8
  
  # Relevance: Is the output relevant to the input prompt?
  # Range: 0.0 - 1.0 (higher = stricter)
  # Recommended: 0.7 for most applications
  relevance_threshold: 0.7
  
  # Hallucination: Does the output contain fabricated information?
  # Range: 0.0 - 1.0 (lower = stricter, less hallucination allowed)
  # Recommended: 0.1 for factual applications, 0.3 for creative tasks
  hallucination_threshold: 0.1
  
  # Consistency: Are multiple runs producing similar outputs?
  # Range: 0.0 - 1.0 (higher = stricter)
  # Recommended: 0.8 for deterministic applications
  consistency_threshold: 0.8

# ── Advanced Configuration ─────────────────────────────────────────────────

# Database settings (optional)
# database:
#   path: "llmqg.duckdb"                  # Database file location
#   backup_interval: 3600                 # Backup interval in seconds

# Notification settings (optional)
# notifications:
#   slack:
#     webhook_url_env: "SLACK_WEBHOOK_URL"
#   email:
#     smtp_host: "smtp.gmail.com"
#     smtp_port: 587
#     username_env: "SMTP_USERNAME"
#     password_env: "SMTP_PASSWORD"

# Logging settings (optional)
# logging:
#   level: "INFO"                         # DEBUG, INFO, WARNING, ERROR
#   file: "llmq.log"                      # Log file path
#   max_size: "10MB"                      # Max log file size
#   backup_count: 5                       # Number of backup files

# ============================================================================
# Usage Examples:
# ============================================================================
#
# Basic evaluation:
#   llmq eval --provider groq
#
# Compare providers:
#   llmq eval --provider groq
#   llmq eval --provider openai
#   llmq compare
#
# CI integration with strict thresholds:
#   llmq eval --provider openai --fail-on-gate
#
# Custom model:
#   llmq eval --provider openai --model gpt-4
#
# ============================================================================